# LNN


A Powerful, Modular, and Easy-to-Use Library for Liquid Neural Networks built on PyTorch

ğŸš€ Overview

Liquid Neural Networks (LNNs) are dynamic neural networks that adapt their internal structure in real-time, inspired by neuroscience. They utilize continuous-time differential equations, allowing efficient adaptation and learning, particularly suited for sequential, temporal, and dynamically evolving datasets.

This library provides a robust and modular implementation of Liquid Neural Networks, making them as easy to use as traditional deep learning layers. Whether you're researching dynamic neural architectures or deploying them into production, LNN offers flexible, efficient, and powerful tools.

ğŸ¯ Why Use This Library?

Modularity: Easily customizable layers and models.

Flexibility: Numerous configuration options (learnable time constants, solvers, activations).

Ease of Use: Clean and intuitive API, suitable for rapid prototyping.

Performance: Optimized for efficient training and inference.

Integration: Seamlessly integrates into your existing PyTorch workflows.

ğŸ“‚ Repository Structure

lnn/
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ liquid.py           # Core Liquid Neural Network layer
â”‚   â”œâ”€â”€ neural_ode.py       # Neural ODE integration
â”‚   â”œâ”€â”€ recurrent_liquid.py # Liquid layer with recurrent capabilities
â”‚   â””â”€â”€ utils.py            # Layer-specific utilities
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ liquid_rnn.py       # Liquid RNN model
â”‚   â”œâ”€â”€ liquid_classifier.py# Classification model using Liquid layers
â”‚   â””â”€â”€ liquid_forecaster.py# Forecasting model using Liquid layers
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ activations.py      # Custom activation functions
â”‚   â”œâ”€â”€ initializations.py  # Weight initialization utilities
â”‚   â””â”€â”€ visualization.py    # Tools for visualizing neuron dynamics
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_usage.py      # Basic implementation example
â”‚   â””â”€â”€ advanced_configuration.py # Advanced setup examples
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_layers.py      # Unit tests for layers
â”‚   â””â”€â”€ test_models.py      # Unit tests for models
â”‚
â”œâ”€â”€ setup.py                # Installation script
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md               # This document

ğŸ›  Installation

Install via PyPI (coming soon):

pip install lnn

Or directly from GitHub:

git clone https://github.com/yourusername/lnn.git
cd lnn
pip install .

ğŸ”¥ Quick Start

Here's how easily you can start:

import torch
from lnn.models.liquid_classifier import LiquidClassifier

# Instantiate the model
model = LiquidClassifier(
    input_dim=5,
    hidden_dim=64,
    num_classes=3,
    time_steps=50,
    activation='tanh',
    learn_tau=True,
    solver='rk4'
)

# Dummy input data
inputs = torch.randn(16, 50, 5)  # [Batch, Time, Features]

# Forward pass
outputs = model(inputs)
print(outputs.shape)  # Output shape: [16, 3]

ğŸ“– Documentation

Layers Documentation

Models Documentation

Examples and Tutorials

(Documentation pages coming soon!)

ğŸŒŸ Contributing

We welcome contributions! To contribute, please:

Fork the repository.

Create a new feature branch (git checkout -b feature/amazing-feature).

Commit your changes (git commit -m 'Add amazing feature').

Push to your branch (git push origin feature/amazing-feature).

Open a Pull Request.

ğŸ“ Citation

If you find this library helpful in your research, please cite:

@misc{your2025lnn,
  author = {Your Name},
  title = {Liquid Neural Networks (LNN) Library},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/yourusername/lnn}}
}

ğŸ“§ Contact

For questions, feature requests, or discussions, reach out:

GitHub Issues: https://github.com/yourusername/lnn/issues

Email: your.email@example.com

ğŸ“œ License

This project is licensed under the MIT License. See LICENSE for details.

Built with â¤ï¸ for the AI and Machine Learning Community.
